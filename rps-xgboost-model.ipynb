{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<h1 align='center', style='padding:5%; color:white; background:#646464;'>Contents</h1>\n\n## XG Boost agents\n* [Agent: Hit The Last Own Action](#1)\n* [Agent: Rock](#2)\n* [Agent: Paper](#3)\n* [Agent: Scissors](#4)\n* [Agent: Copy Opponent](#5)\n* [Agent: Reactionary](#6)\n* [Agent: Counter Reactionary](#7)\n* [Agent: Statistical](#8)\n* [Agent: Nash Equilibrium](#9)\n* [Agent: Statistical Prediction](#10)\n\n### Performance\n* [Results](#103)\n* [Review](#104)\n","metadata":{}},{"cell_type":"code","source":"%%writefile random_forest_random.py\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = xgb1 = XGBClassifier(\n learning_rate =0.01,\n n_estimators=25,\n nthread=4)\n    model.fit(X_train, y_train)\n\n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Hit The Last Own Action<h1>","metadata":{}},{"cell_type":"markdown","source":"The idea of the agent:\n\n- A lot of agents use a simple baseline - copy the last action of the opponent.   \n- That's why we can simply hit our last actions (new action of the opponent)","metadata":{}},{"cell_type":"code","source":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Rock<h1>","metadata":{}},{"cell_type":"markdown","source":"The idea of the agent:\n\n- Always uses Rock action","metadata":{}},{"cell_type":"code","source":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Paper<h1>","metadata":{}},{"cell_type":"markdown","source":"The idea of this Agent:\n\n- Always uses Paper action","metadata":{}},{"cell_type":"code","source":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Scissors<h1>","metadata":{}},{"cell_type":"markdown","source":"The idea of the Agent:\n\n- Always uses Scissors action","metadata":{}},{"cell_type":"code","source":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Copy Opponent<h1>","metadata":{}},{"cell_type":"markdown","source":"The idea of the agent:\n\n- Copy the last action of the opponent","metadata":{}},{"cell_type":"code","source":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Reactionary<h1>","metadata":{}},{"cell_type":"markdown","source":"The idea of the agent:\n\n- Hit the last action of the opponent","metadata":{}},{"cell_type":"code","source":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Counter Reactionary<h1>","metadata":{}},{"cell_type":"markdown","source":"The idea of this Agent:\n\n- Hit the counter to the last action of the opponent","metadata":{}},{"cell_type":"code","source":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Statistical<h1>","metadata":{}},{"cell_type":"code","source":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Nash Equilibrium<h1>","metadata":{}},{"cell_type":"markdown","source":"Refference - [Rock Paper Scissors - Nash Equilibrium Strategy](https://www.kaggle.com/ihelon/rock-paper-scissors-nash-equilibrium-strategy)\n\nNash Equilibrium Strategy (always random)","metadata":{}},{"cell_type":"code","source":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n<h1 align='center' style='padding:5%; background:#646464; color:white'>Agent: Statistical Prediction<h1>","metadata":{}},{"cell_type":"code","source":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"103\"></a>\n<h1 align='center' style='padding:10%; background-color:#646464; color:white;'>Results<h1>","metadata":{}},{"cell_type":"code","source":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=range(10),\n)\n\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Final Reward Score', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_review=pd.DataFrame()\ndf_review['Won'] = df_scores.select_dtypes(include='int').gt(0).sum(axis=1)\ndf_review['Tie'] = df_scores.select_dtypes(include='int').eq(0).sum(axis=1)\ndf_review['Lost'] = df_scores.select_dtypes(include='int').lt(0).sum(axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 10))\nsns.heatmap(\n    df_review, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Total games Won-Tie-Lost', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"104\"></a>\n<h1 align='center' style='padding:10%; background-color:#646464; color:white;'>Review<h1>","metadata":{}},{"cell_type":"markdown","source":"* `Random Forest Random` can identify the patterns of all simple agents in 5 actions or less.\n* `Statistical` is an easy opponent for `Random Forest Random` and performs better almost every time.\n* Luck is crucial for the outcome over `Opponent Transition Matrix`, `Decision Tree Classifier` and `Statistical Prediction` as the results can vary a lot over matches.\n* Final conclusion is that `Random Forest Classifiers` can be used to predict opponents actions on `Rock-Paper-Scissors` but advanced `defensive` mechanisms are required when the pattern is identified by the opponent.\n\n**Disclaimer: The above review is done on multiple runs of this notebook and is revised after viewing the submmisions in the competetion  after its was ended.**","metadata":{}}]}
